{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/25/92/ee1d7a00bb6b8c55755d4984fd82608603a3cc59959245068ce32e7fb808/scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from scikit-learn) (2.2.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/61/d8/84da3fffefb6c7d5a16968fe5b9f24c98606b165bb801bb0b8bc3985200f/scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl.metadata\n",
      "  Downloading scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/91/29/df4b9b42f2be0b623cbd5e2140cafcaa2bef0759a00b7b70104dcfe2fb51/joblib-1.4.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.2 threadpoolctl-3.5.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN as SklearnDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(iris_dataset_path)\n",
    "ai_global_index_df = pd.read_csv(ai_global_index_path)\n",
    "global_earthquake_data_df = pd.read_csv(global_earthquake_data_path)\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": iris_df,\n",
    "    \"ai_global_index\": ai_global_index_df,\n",
    "    \"global_earthquake\": global_earthquake_data_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBSCANFromScratch:\n",
    "    def __init__(self, eps=0.5, min_pts=5):\n",
    "        self.eps = eps\n",
    "        self.min_pts = min_pts\n",
    "        self.labels_ = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        n = len(X)\n",
    "        self.labels_ = np.full(n, -1)  # -1 = noise\n",
    "        visited = np.zeros(n, dtype=bool)\n",
    "        cluster_id = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            visited[i] = True\n",
    "            neighbors = self.region_query(X, i)\n",
    "\n",
    "            if len(neighbors) < self.min_pts:\n",
    "                self.labels_[i] = -1  # noise\n",
    "            else:\n",
    "                self.expand_cluster(X, i, neighbors, cluster_id, visited)\n",
    "                cluster_id += 1\n",
    "\n",
    "    def expand_cluster(self, X, point_idx, neighbors, cluster_id, visited):\n",
    "        self.labels_[point_idx] = cluster_id\n",
    "        i = 0\n",
    "        while i < len(neighbors):\n",
    "            neighbor_idx = neighbors[i]\n",
    "            if not visited[neighbor_idx]:\n",
    "                visited[neighbor_idx] = True\n",
    "                neighbor_neighbors = self.region_query(X, neighbor_idx)\n",
    "                if len(neighbor_neighbors) >= self.min_pts:\n",
    "                    neighbors += [n for n in neighbor_neighbors if n not in neighbors]\n",
    "            if self.labels_[neighbor_idx] == -1:\n",
    "                self.labels_[neighbor_idx] = cluster_id\n",
    "            i += 1\n",
    "\n",
    "    def region_query(self, X, idx):\n",
    "        distances = np.linalg.norm(X - X[idx], axis=1)\n",
    "        return list(np.where(distances <= self.eps)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing iris dataset\n",
      "Custom DBSCAN Labels: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1 -1 -1  1 -1 -1  1 -1  1  1  1  1  1 -1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1  1  1\n",
      "  1  1 -1  1 -1  1  1  1  1 -1 -1 -1 -1 -1  1  1  1  1 -1  1  1 -1 -1 -1\n",
      "  1  1 -1  1  1 -1  1  1  1 -1 -1 -1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1]\n",
      "Sklearn DBSCAN Labels: [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0 -1 -1  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1 -1 -1  1 -1 -1  1 -1  1  1  1  1  1 -1  1  1  1\n",
      " -1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1 -1  1  1  1  1  1 -1  1  1\n",
      "  1  1 -1  1 -1  1  1  1  1 -1 -1 -1 -1 -1  1  1  1  1 -1  1  1 -1 -1 -1\n",
      "  1  1 -1  1  1 -1  1  1  1 -1 -1 -1  1  1  1 -1 -1  1  1  1  1  1  1  1\n",
      "  1  1  1  1 -1  1]\n",
      "\n",
      "Processing ai_global_index dataset\n",
      "Custom DBSCAN Labels: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Sklearn DBSCAN Labels: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "\n",
      "Processing global_earthquake dataset\n",
      "Custom DBSCAN Labels: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Sklearn DBSCAN Labels: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {name} dataset\")\n",
    "\n",
    "    # Handle missing values\n",
    "    df = df.dropna()  # or use df.fillna(df.mean(numeric_only=True))\n",
    "\n",
    "    # Select only numeric columns\n",
    "    X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "    # Normalize\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Run custom DBSCAN\n",
    "    dbscan_custom = DBSCANFromScratch(eps=0.5, min_pts=5)\n",
    "    dbscan_custom.fit(X)\n",
    "    print(\"Custom DBSCAN Labels:\", dbscan_custom.labels_)\n",
    "\n",
    "    # Run sklearn DBSCAN\n",
    "    dbscan_sklearn = SklearnDBSCAN(eps=0.5, min_samples=5)\n",
    "    dbscan_sklearn.fit(X)\n",
    "    print(\"Sklearn DBSCAN Labels:\", dbscan_sklearn.labels_)\n",
    "\n",
    "    # Compare clustering results (NOTE: Cluster numbers might not match; we only check label structure)\n",
    "    same_labels = np.array_equal(dbscan_custom.labels_, dbscan_sklearn.labels_)\n",
    "    results[name] = same_labels\n",
    "\n",
    "# Save results\n",
    "results = pd.Series(results)\n",
    "results.to_csv(\"./../results/01-dbscan.csv\", header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
