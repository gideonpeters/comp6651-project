{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5b2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921c2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": pd.read_csv(iris_dataset_path),\n",
    "    \"ai_global_index\": pd.read_csv(ai_global_index_path),\n",
    "    \"global_earthquake\": pd.read_csv(global_earthquake_data_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "997b0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7af6a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def affinity_propagation(S, damping=0.5, preference=None, max_iter=200, convergence_iter=15):\n",
    "    \"\"\"\n",
    "    Custom Affinity Propagation implementation with full parameter support.\n",
    "    \n",
    "    Args:\n",
    "        S: Similarity matrix (precomputed, shape [n_samples, n_samples])\n",
    "        damping: Damping factor (0.5-1.0), same as sklearn's 'damping'\n",
    "        preference: Preference for exemplars (if None, uses median(S) like sklearn)\n",
    "        max_iter: Maximum iterations\n",
    "        convergence_iter: Early stopping if no change\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of {exemplar_index: [list_of_member_indices]}\n",
    "    \"\"\"\n",
    "    n = S.shape[0]\n",
    "    \n",
    "    # Set preference (like sklearn's default)\n",
    "    if preference is None:\n",
    "        preference = np.median(S)\n",
    "    \n",
    "    # Initialize diagonal of S with preferences\n",
    "    np.fill_diagonal(S, preference)\n",
    "    \n",
    "    # Initialize messages\n",
    "    R = np.zeros((n, n))  # Responsibilities\n",
    "    A = np.zeros((n, n))  # Availabilities\n",
    "    \n",
    "    exemplars_prev = np.zeros(n, dtype=int)\n",
    "    stable_count = 0\n",
    "    \n",
    "    for iteration in range(max_iter):\n",
    "        # ===== Responsibilities Update =====\n",
    "        # Compute AS = A + S (for stability)\n",
    "        AS = A + S\n",
    "        \n",
    "        # Find max values (excluding diagonal)\n",
    "        max_indices = np.argmax(AS, axis=1)\n",
    "        max_values = AS[np.arange(n), max_indices]\n",
    "        \n",
    "        # Set max values to -inf for secondary max calculation\n",
    "        AS_copy = AS.copy()\n",
    "        AS_copy[np.arange(n), max_indices] = -np.inf\n",
    "        secondary_max = np.max(AS_copy, axis=1)\n",
    "        \n",
    "        # Update responsibilities with damping\n",
    "        R_new = S - max_values[:, np.newaxis]\n",
    "        R_new[np.arange(n), max_indices] = S[np.arange(n), max_indices] - secondary_max\n",
    "        R = damping * R + (1 - damping) * R_new\n",
    "        \n",
    "        # ===== Availabilities Update =====\n",
    "        # Compute positive responsibilities\n",
    "        Rp = np.maximum(R, 0)\n",
    "        np.fill_diagonal(Rp, R.diagonal())  # Keep self-responsibility\n",
    "        \n",
    "        # Update availabilities with damping\n",
    "        A_new = np.sum(Rp, axis=0) - Rp\n",
    "        A_new = np.minimum(A_new, 0)\n",
    "        \n",
    "        # Self-availability update\n",
    "        A_new.flat[::n+1] = np.sum(Rp, axis=0) - np.diag(Rp)\n",
    "        \n",
    "        A = damping * A + (1 - damping) * A_new\n",
    "        \n",
    "        # ===== Convergence Check =====\n",
    "        current_exemplars = np.argmax(A + R, axis=1)\n",
    "        if np.array_equal(current_exemplars, exemplars_prev):\n",
    "            stable_count += 1\n",
    "            if stable_count >= convergence_iter:\n",
    "                break\n",
    "        else:\n",
    "            stable_count = 0\n",
    "            \n",
    "        exemplars_prev = current_exemplars\n",
    "    \n",
    "    # ===== Cluster Assignment =====\n",
    "    exemplars = np.unique(current_exemplars)\n",
    "    clusters = {e: [] for e in exemplars}\n",
    "    \n",
    "    for i in range(n):\n",
    "        clusters[current_exemplars[i]].append(i)\n",
    "    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dc5737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing ai_global_index dataset ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/3331829190.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;31m# Usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_optimum_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'ai_global_index'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ai_global_index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/3331829190.py\u001b[0m in \u001b[0;36mfind_optimum_metrics\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m     20\u001b[0m         }\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdamping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreference\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'damping'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nTesting damping={damping}, preference={preference}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'product' is not defined"
     ]
    }
   ],
   "source": [
    "def find_optimum_metrics(datasets):\n",
    "    results = {}\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n=== Processing {name} dataset ===\")\n",
    "        df = df.dropna()\n",
    "        X = StandardScaler().fit_transform(df.select_dtypes(include=[np.number]))\n",
    "        \n",
    "        param_grid = {\n",
    "            'damping': [0.5, 0.7, 0.9],\n",
    "            'preference': [-200, -100, -50, -10]  # Wider range for preference\n",
    "        }\n",
    "        \n",
    "        best_params = None\n",
    "        best_metrics = {\n",
    "            'Silhouette Score': -1,\n",
    "            'Davies-Bouldin Index': float('inf'),\n",
    "            'Calinski-Harabasz Index': -1,\n",
    "            'n_clusters': 0\n",
    "        }\n",
    "        \n",
    "        for damping, preference in product(param_grid['damping'], param_grid['preference']):\n",
    "            print(f\"\\nTesting damping={damping}, preference={preference}\")\n",
    "            \n",
    "            try:\n",
    "                ap = AffinityPropagation(damping=damping, preference=preference, random_state=42)\n",
    "                labels = ap.fit_predict(X)\n",
    "                \n",
    "                # Handle noise points (label = -1)\n",
    "                unique_labels = set(labels)\n",
    "                n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "                \n",
    "                if n_clusters < 2:\n",
    "                    print(f\"Only {n_clusters} clusters formed - skipping\")\n",
    "                    continue\n",
    "                    \n",
    "                print(f\"Formed {n_clusters} clusters\")\n",
    "                \n",
    "                metrics = {\n",
    "                    'Silhouette Score': silhouette_score(X, labels),\n",
    "                    'Davies-Bouldin Index': davies_bouldin_score(X, labels),\n",
    "                    'Calinski-Harabasz Index': calinski_harabasz_score(X, labels),\n",
    "                    'n_clusters': n_clusters\n",
    "                }\n",
    "                \n",
    "                print(\"Current metrics:\", {k: round(v, 4) if isinstance(v, float) else v \n",
    "                                         for k, v in metrics.items()})\n",
    "                \n",
    "                # Update best metrics (prioritizing Silhouette Score)\n",
    "                if metrics['Silhouette Score'] > best_metrics['Silhouette Score']:\n",
    "                    best_params = {'damping': damping, 'preference': preference}\n",
    "                    best_metrics.update(metrics)\n",
    "                    print(\"⭐ New best parameters found!\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Clustering failed: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        # Store and display final results\n",
    "        results[name] = {\n",
    "            'Best Parameters': best_params,\n",
    "            'Best Metrics': best_metrics\n",
    "        }\n",
    "        \n",
    "        print(\"\\n=== Final Results ===\")\n",
    "        print(f\"Dataset: {name}\")\n",
    "        print(\"Optimal Parameters:\", best_params)\n",
    "        print(\"Best Metrics:\")\n",
    "        for k, v in best_metrics.items():\n",
    "            print(f\"- {k}: {round(v, 4) if isinstance(v, float) else v}\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "results = find_optimum_metrics({'ai_global_index': datasets.get('ai_global_index')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a12c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing iris dataset ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/678027664.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_optimum_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'iris'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'iris'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/3331829190.py\u001b[0m in \u001b[0;36mfind_optimum_metrics\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m     20\u001b[0m         }\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdamping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreference\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'damping'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nTesting damping={damping}, preference={preference}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'product' is not defined"
     ]
    }
   ],
   "source": [
    "results = find_optimum_metrics({'iris': datasets.get('iris')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee80d2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing global_earthquake dataset ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/406821191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_optimum_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'global_earthquake'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'global_earthquake'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_28236/3331829190.py\u001b[0m in \u001b[0;36mfind_optimum_metrics\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m     20\u001b[0m         }\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdamping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreference\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'damping'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preference'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"\\nTesting damping={damping}, preference={preference}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'product' is not defined"
     ]
    }
   ],
   "source": [
    "results = find_optimum_metrics({'global_earthquake': datasets.get('global_earthquake')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e64990a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing dataset: iris\n",
      "==================================================\n",
      "Using parameters - Damping: 0.7, Preference: -100\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 2 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.5802\n",
      "         DBI: 0.5976\n",
      "         CHI: 248.9034\n",
      "  n_clusters: 2\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.5802\n",
      "         DBI: 0.5976\n",
      "         CHI: 248.9034\n",
      "  n_clusters: 2\n",
      "\n",
      "==================================================\n",
      "Processing dataset: ai_global_index\n",
      "==================================================\n",
      "Using parameters - Damping: 0.9, Preference: -10\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 8 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 0.0143\n",
      "  Silhouette: 0.2562\n",
      "         DBI: 0.8951\n",
      "         CHI: 33.1685\n",
      "  n_clusters: 8\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 0.0143\n",
      "  Silhouette: 0.7358\n",
      "         DBI: 0.1731\n",
      "         CHI: 25.9479\n",
      "  n_clusters: 2\n",
      "\n",
      "==================================================\n",
      "Processing dataset: global_earthquake\n",
      "==================================================\n",
      "Using parameters - Damping: 0.5, Preference: -200\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 2 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.1569\n",
      "         DBI: 2.4683\n",
      "         CHI: 8.8883\n",
      "  n_clusters: 2\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.1569\n",
      "         DBI: 2.4683\n",
      "         CHI: 8.8883\n",
      "  n_clusters: 2\n",
      "\n",
      "Saved results to affinity_comparison_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (adjusted_rand_score, silhouette_score, \n",
    "                           davies_bouldin_score, calinski_harabasz_score)\n",
    "\n",
    "def compare_affinity_implementations(datasets, best_parameters):\n",
    "    \"\"\"\n",
    "    Compare custom and sklearn implementations of Affinity Propagation.\n",
    "    \n",
    "    Args:\n",
    "        datasets: Dictionary of {dataset_name: dataframe}\n",
    "        best_parameters: Dictionary of {dataset_name: parameters_dict}\n",
    "                        where parameters_dict contains 'damping' and 'preference'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing comparison results for all datasets\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"\\n{'='*50}\\nProcessing dataset: {name}\\n{'='*50}\")\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Get best parameters for this dataset\n",
    "        params = best_parameters.get(name, {})\n",
    "        damping = params.get('damping', 0.5)\n",
    "        preference = params.get('preference', None)\n",
    "        \n",
    "        print(f\"Using parameters - Damping: {damping}, Preference: {preference}\")\n",
    "\n",
    "        # Extract numerical features and normalize\n",
    "        X = df.select_dtypes(include=[np.number]).values\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        S = -np.square(np.linalg.norm(X[:, None] - X[None, :], axis=2))\n",
    "        \n",
    "        # If preference is None, use median similarity (like sklearn default)\n",
    "        if preference is None:\n",
    "            preference = np.median(S)\n",
    "            print(f\"Using computed preference: {preference:.2f}\")\n",
    "        \n",
    "        results = {'Custom Affinity': {}, 'Sklearn Affinity': {}}\n",
    "        \n",
    "        # Custom Implementation\n",
    "        try:\n",
    "            print(\"\\nRunning custom Affinity Propagation...\")\n",
    "            custom_clusters = affinity_propagation(S, damping=damping, preference=preference)\n",
    "            custom_labels = np.zeros(len(X), dtype=int)\n",
    "            \n",
    "            for cluster_id, members in enumerate(custom_clusters.values()):\n",
    "                for idx in members:\n",
    "                    custom_labels[idx] = cluster_id\n",
    "            \n",
    "            n_custom_clusters = len(set(custom_labels))\n",
    "            print(f\"Custom AP created {n_custom_clusters} clusters\")\n",
    "            \n",
    "            if n_custom_clusters < 2 or n_custom_clusters >= len(X):\n",
    "                raise ValueError(f\"Invalid cluster count: {n_custom_clusters}\")\n",
    "                \n",
    "            results['Custom Affinity'] = {\n",
    "                \"ARI\": None,  # Will fill after sklearn runs\n",
    "                \"Silhouette\": silhouette_score(X, custom_labels),\n",
    "                \"DBI\": davies_bouldin_score(X, custom_labels),\n",
    "                \"CHI\": calinski_harabasz_score(X, custom_labels),\n",
    "                \"n_clusters\": n_custom_clusters\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Custom AP failed: {str(e)}\")\n",
    "            results['Custom Affinity'] = {\n",
    "                \"ARI\": np.nan, \"Silhouette\": np.nan, \n",
    "                \"DBI\": np.nan, \"CHI\": np.nan, \"n_clusters\": 0\n",
    "            }\n",
    "\n",
    "        # Sklearn Implementation\n",
    "        try:\n",
    "            print(\"\\nRunning sklearn Affinity Propagation...\")\n",
    "            sklearn_ap = AffinityPropagation(\n",
    "                affinity='precomputed',\n",
    "                damping=damping,\n",
    "                preference=preference,\n",
    "                random_state=42\n",
    "            )\n",
    "            sklearn_labels = sklearn_ap.fit_predict(S)\n",
    "            \n",
    "            n_sklearn_clusters = len(set(sklearn_labels))\n",
    "            print(f\"Sklearn AP created {n_sklearn_clusters} clusters\")\n",
    "            \n",
    "            if n_sklearn_clusters < 2 or n_sklearn_clusters >= len(X):\n",
    "                raise ValueError(f\"Invalid cluster count: {n_sklearn_clusters}\")\n",
    "                \n",
    "            results['Sklearn Affinity'] = {\n",
    "                \"ARI\": adjusted_rand_score(sklearn_labels, custom_labels) if 'custom_labels' in locals() else np.nan,\n",
    "                \"Silhouette\": silhouette_score(X, sklearn_labels),\n",
    "                \"DBI\": davies_bouldin_score(X, sklearn_labels),\n",
    "                \"CHI\": calinski_harabasz_score(X, sklearn_labels),\n",
    "                \"n_clusters\": n_sklearn_clusters\n",
    "            }\n",
    "            \n",
    "            # Update ARI for custom if sklearn succeeded\n",
    "            if 'custom_labels' in locals():\n",
    "                results['Custom Affinity']['ARI'] = adjusted_rand_score(sklearn_labels, custom_labels)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Sklearn AP failed: {str(e)}\")\n",
    "            results['Sklearn Affinity'] = {\n",
    "                \"ARI\": np.nan, \"Silhouette\": np.nan, \n",
    "                \"DBI\": np.nan, \"CHI\": np.nan, \"n_clusters\": 0\n",
    "            }\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nComparison Results:\")\n",
    "        for method in results:\n",
    "            print(f\"\\n{method}:\")\n",
    "            for metric, value in results[method].items():\n",
    "                if isinstance(value, float):\n",
    "                    print(f\"{metric:>12}: {value:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{metric:>12}: {value}\")\n",
    "        \n",
    "        all_results[name] = results\n",
    "\n",
    "    # Save results\n",
    "    results_df = pd.DataFrame.from_dict(\n",
    "        {(dataset, method): metrics \n",
    "         for dataset in all_results \n",
    "         for method, metrics in all_results[dataset].items()},\n",
    "        orient='index'\n",
    "    )\n",
    "    results_df.to_csv(\"./../results/affinity_comparison_metrics.csv\")\n",
    "    print(\"\\nSaved results to affinity_comparison_metrics.csv\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets\n",
    "    \n",
    "\n",
    "    # Define the best parameters for Affinity Propagation\n",
    "    best_parameters = {\n",
    "        'iris': {'damping': 0.7, 'preference': -100},\n",
    "        'ai_global_index': {'damping': 0.9, 'preference': -10},\n",
    "        'global_earthquake': {'damping': 0.5, 'preference': -200},\n",
    "    }\n",
    "\n",
    "    # Compare implementations\n",
    "    results = compare_affinity_implementations(datasets, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d630f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing dataset: iris\n",
      "==================================================\n",
      "Using parameters - Damping: 0.7, Preference: -100\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 2 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.5802\n",
      "         DBI: 0.5976\n",
      "         CHI: 248.9034\n",
      "  n_clusters: 2\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.5802\n",
      "         DBI: 0.5976\n",
      "         CHI: 248.9034\n",
      "  n_clusters: 2\n",
      "\n",
      "==================================================\n",
      "Processing dataset: ai_global_index\n",
      "==================================================\n",
      "Using parameters - Damping: 0.9, Preference: -10\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 8 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 0.0143\n",
      "  Silhouette: 0.2562\n",
      "         DBI: 0.8951\n",
      "         CHI: 33.1685\n",
      "  n_clusters: 8\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 0.0143\n",
      "  Silhouette: 0.7358\n",
      "         DBI: 0.1731\n",
      "         CHI: 25.9479\n",
      "  n_clusters: 2\n",
      "\n",
      "==================================================\n",
      "Processing dataset: global_earthquake\n",
      "==================================================\n",
      "Using parameters - Damping: 0.5, Preference: -200\n",
      "\n",
      "Running custom Affinity Propagation...\n",
      "Custom AP created 2 clusters\n",
      "\n",
      "Running sklearn Affinity Propagation...\n",
      "Sklearn AP created 2 clusters\n",
      "\n",
      "Comparison Results:\n",
      "\n",
      "Custom Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.1569\n",
      "         DBI: 2.4683\n",
      "         CHI: 8.8883\n",
      "  n_clusters: 2\n",
      "\n",
      "Sklearn Affinity:\n",
      "         ARI: 1.0000\n",
      "  Silhouette: 0.1569\n",
      "         DBI: 2.4683\n",
      "         CHI: 8.8883\n",
      "  n_clusters: 2\n",
      "\n",
      "Saved results to affinity_comparison_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets\n",
    "    \n",
    "\n",
    "    # Define the best parameters for Affinity Propagation\n",
    "    best_parameters = {\n",
    "        'iris': {'damping': 0.7, 'preference': -100},\n",
    "        'ai_global_index': {'damping': 0.9, 'preference': -10},\n",
    "        'global_earthquake': {'damping': 0.5, 'preference': -200},\n",
    "    }\n",
    "\n",
    "    # Prepare datasets dictionary\n",
    "    \n",
    "\n",
    "    # Compare implementations\n",
    "    results = compare_affinity_implementations(datasets, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc24e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
