{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchy Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": pd.read_csv(iris_dataset_path),\n",
    "    \"ai_global_index\": pd.read_csv(ai_global_index_path),\n",
    "    \"global_earthquake\": pd.read_csv(global_earthquake_data_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(iris_dataset_path)\n",
    "ai_global_index_df = pd.read_csv(ai_global_index_path)\n",
    "global_earthquake_data_df = pd.read_csv(global_earthquake_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH Implementation (Based on our Algorithm - see report/Part-1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, points):\n",
    "        self.points = points\n",
    "\n",
    "    def merge(self, other_cluster):\n",
    "        return Cluster(self.points + other_cluster.points)\n",
    "\n",
    "    def centroid(self):\n",
    "        return np.mean(self.points, axis=0)\n",
    "\n",
    "\n",
    "def compute_distance(cluster1, cluster2, linkage='single'):\n",
    "    if linkage == 'single':\n",
    "        return min(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'complete':\n",
    "        return max(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'average':\n",
    "        distances = [euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points]\n",
    "        return np.mean(distances)\n",
    "\n",
    "\n",
    "def hac_custom(data, k, linkage='single'):\n",
    "    clusters = [Cluster([point]) for point in data]\n",
    "\n",
    "    while len(clusters) > k:\n",
    "        min_distance = float('inf')\n",
    "        to_merge = (None, None)\n",
    "\n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(i + 1, len(clusters)):\n",
    "                distance = compute_distance(clusters[i], clusters[j], linkage)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    to_merge = (i, j)\n",
    "\n",
    "        cluster1, cluster2 = to_merge\n",
    "        new_cluster = clusters[cluster1].merge(clusters[cluster2])\n",
    "        clusters = [c for idx, c in enumerate(clusters) if idx not in (cluster1, cluster2)]\n",
    "        clusters.append(new_cluster)\n",
    "\n",
    "    labels = np.zeros(len(data), dtype=int)\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point in cluster.points:\n",
    "            point_index = np.where((data == point).all(axis=1))[0][0]\n",
    "            labels[point_index] = cluster_idx\n",
    "\n",
    "    return clusters, labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    data = np.array([\n",
    "        [1, 2], [1, 4], [1, 0],\n",
    "        [10, 2], [10, 4], [10, 0]\n",
    "    ])\n",
    "\n",
    "    # Number of clusters\n",
    "    k = 2\n",
    "\n",
    "    # Run custom HAC\n",
    "    clusters, labels = hac_custom(data, k, linkage='average')\n",
    "\n",
    "    print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dataset1, Type: <class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_13552/2737873617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mcustom_linkage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ward'\u001b[0m  \u001b[1;31m# Choose linkage method ('ward', 'single', 'complete', 'average')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mcustom_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhac_custom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_linkage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ANJOLA~1\\AppData\\Local\\Temp/ipykernel_13552/1313680593.py\u001b[0m in \u001b[0;36mhac_custom\u001b[1;34m(data, k, linkage)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinkage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mdistance\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                     \u001b[0mmin_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                     \u001b[0mto_merge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import Birch as SklearnBIRCH\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "# Assuming birch_clustering is defined elsewhere\n",
    "def birch_clustering(X, threshold, branching_factor, n_clusters):\n",
    "    # Dummy function for custom BIRCH clustering\n",
    "    # Replace this with actual logic\n",
    "    centroids = np.random.rand(n_clusters, X.shape[1])\n",
    "    return None, centroids\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Sample datasets (replace with actual data loading)\n",
    "datasets = {\n",
    "    \"dataset1\": pd.DataFrame(np.random.rand(100, 5)),\n",
    "    \"dataset2\": pd.DataFrame(np.random.rand(200, 5))\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"Dataset: {name}, Type: {type(df)}\")\n",
    "\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Extract numerical features\n",
    "    X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "    # Normalize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Run the custom BIRCH implementation\n",
    "    threshold = 0.01  # Adjust as needed\n",
    "    branching_factor = 10  # Adjust as needed\n",
    "    n_clusters = 3  # Adjust as needed\n",
    "\n",
    "    cf_tree, custom_centroids = birch_clustering(X, threshold, branching_factor, n_clusters)\n",
    "    custom_labels = np.zeros(len(X))  # Placeholder for custom labels\n",
    "\n",
    "    # Assign labels based on closest centroid\n",
    "    for i, point in enumerate(X):\n",
    "        distances = [euclidean(point, centroid) for centroid in custom_centroids]\n",
    "        custom_labels[i] = np.argmin(distances)\n",
    "\n",
    "    print(f\"Custom BIRCH Centroids for {name}:\")\n",
    "    for i, centroid in enumerate(custom_centroids):\n",
    "        print(f\"Cluster {i + 1}: {centroid}\")\n",
    "\n",
    "    print(f\"\\nCustom BIRCH Labels for {name}: {custom_labels}\")\n",
    "\n",
    "    # Run the sklearn BIRCH implementation\n",
    "    sklearn_birch = SklearnBIRCH(threshold=threshold, branching_factor=branching_factor, n_clusters=n_clusters)\n",
    "    sklearn_labels = sklearn_birch.fit_predict(X)\n",
    "\n",
    "    print(f\"Sklearn BIRCH Labels for {name}: {sklearn_labels}\")\n",
    "\n",
    "    # Compare the results using Adjusted Rand Index (ARI)\n",
    "    ari_score = adjusted_rand_score(custom_labels, sklearn_labels)\n",
    "    results[name] = ari_score\n",
    "    print(f\"Adjusted Rand Index (ARI) for {name}: {ari_score}\")\n",
    "\n",
    "# Store results\n",
    "results = pd.Series(results)\n",
    "results.to_csv(\"./../results/birch_comparison.csv\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
