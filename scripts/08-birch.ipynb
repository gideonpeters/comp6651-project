{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRCH Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import Birch as SklearnBIRCH\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": iris_df,\n",
    "    \"ai_global_index\": ai_global_index_df,\n",
    "    \"global_earthquake\": global_earthquake_data_df\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(iris_dataset_path)\n",
    "ai_global_index_df = pd.read_csv(ai_global_index_path)\n",
    "global_earthquake_data_df = pd.read_csv(global_earthquake_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH Implementation (Based on our Algorithm - see report/Part-1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom BIRCH Centroids for ai_global_index:\n",
      "Cluster 1: [1. 2.]\n",
      "Cluster 2: [10.  2.]\n",
      "\n",
      "Custom BIRCH Labels for ai_global_index:\n",
      "[0 0 0 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusteringFeature:\n",
    "    def __init__(self, point):\n",
    "        self.N = 1  # Number of points\n",
    "        self.LS = np.array(point)  # Linear sum of points\n",
    "        self.SS = np.array(point) ** 2  # Squared sum of points\n",
    "\n",
    "    def add_point(self, point):\n",
    "        self.N += 1\n",
    "        self.LS += np.array(point)\n",
    "        self.SS += np.array(point) ** 2\n",
    "\n",
    "    def centroid(self):\n",
    "        return self.LS / self.N\n",
    "    \n",
    "    def radius(self):\n",
    "        return np.linalg.norm(self.LS / self.N)\n",
    "\n",
    "    def can_absorb(self, point, threshold):\n",
    "        new_centroid = (self.LS + np.array(point)) / (self.N + 1)\n",
    "        distance = euclidean(new_centroid, point)\n",
    "        return distance <= threshold\n",
    "\n",
    "\n",
    "class CFNode:\n",
    "    def __init__(self, branching_factor):\n",
    "        self.branching_factor = branching_factor\n",
    "        self.children = []\n",
    "        self.cf = None\n",
    "\n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def insert(self, point, threshold):\n",
    "        if self.is_leaf():\n",
    "            if self.cf is None:\n",
    "                self.cf = ClusteringFeature(point)\n",
    "            elif self.cf.can_absorb(point, threshold):\n",
    "                self.cf.add_point(point)\n",
    "            else:\n",
    "                self.split(point, threshold)\n",
    "        else:\n",
    "            # Filter out nodes with `cf` as None\n",
    "            valid_children = [child for child in self.children if child.cf is not None]\n",
    "\n",
    "            if valid_children:\n",
    "                closest_child = min(valid_children, key=lambda child: euclidean(child.cf.centroid(), point))\n",
    "                closest_child.insert(point, threshold)\n",
    "\n",
    "    def split(self, point, threshold):\n",
    "        points = [self.cf.centroid(), point]\n",
    "        kmeans = KMeans(n_clusters=2, n_init=10).fit(points)\n",
    "\n",
    "        new_node1 = CFNode(self.branching_factor)\n",
    "        new_node2 = CFNode(self.branching_factor)\n",
    "\n",
    "        # Assign points to the new nodes\n",
    "        for i, cluster_id in enumerate(kmeans.labels_):\n",
    "            target_node = new_node1 if cluster_id == 0 else new_node2\n",
    "            target_node.cf = ClusteringFeature(points[i])\n",
    "\n",
    "        # Replace current node with child nodes\n",
    "        self.children = [new_node1, new_node2]\n",
    "        self.cf = None  # Reset the CF for this node\n",
    "\n",
    "\n",
    "class CFTree:\n",
    "    def __init__(self, threshold, branching_factor):\n",
    "        self.threshold = threshold\n",
    "        self.branching_factor = branching_factor\n",
    "        self.root = CFNode(branching_factor)\n",
    "\n",
    "    def insert(self, point):\n",
    "        self.root.insert(point, self.threshold)\n",
    "        \n",
    "    def get_leaves(self, node=None):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        if node.is_leaf():\n",
    "            return [node.cf]\n",
    "        leaves = []\n",
    "        for child in node.children:\n",
    "            leaves.extend(self.get_leaves(child))\n",
    "        return leaves\n",
    "\n",
    "\n",
    "def birch_clustering(data, threshold, branching_factor, n_clusters):\n",
    "    # Phase 1: Build the CF Tree\n",
    "    cf_tree = CFTree(threshold, branching_factor)\n",
    "    for point in data:\n",
    "        cf_tree.insert(point)\n",
    "\n",
    "    # Phase 2: Cluster the Leaves\n",
    "    leaves = cf_tree.get_leaves()\n",
    "    leaf_centroids = [leaf.centroid() for leaf in leaves]\n",
    "\n",
    "    if len(leaf_centroids) < 2:\n",
    "        print(\"Warning: Only one leaf node found. Cannot perform clustering.\")\n",
    "        return cf_tree, leaf_centroids\n",
    "\n",
    "    # Agglomerative clustering on leaf centroids\n",
    "    agg_clustering = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    cluster_labels = agg_clustering.fit_predict(leaf_centroids)\n",
    "\n",
    "    # Compute final cluster centroids\n",
    "    cluster_centroids = []\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = [leaf_centroids[j] for j in range(len(leaf_centroids)) if cluster_labels[j] == i]\n",
    "        cluster_centroids.append(np.mean(cluster_points, axis=0))\n",
    "\n",
    "    return cf_tree, cluster_centroids\n",
    "\n",
    "\n",
    "# Custom function to assign cluster to each data point\n",
    "def assign_clusters(data_points, centroids):\n",
    "    distances = np.linalg.norm(data_points[:, np.newaxis] - centroids, axis=2)\n",
    "    cluster_labels = np.argmin(distances, axis=1)\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    data = np.array([\n",
    "        [1, 2], [1, 4], [1, 0],\n",
    "        [10, 2], [10, 4], [10, 0]\n",
    "    ])\n",
    "\n",
    "    # Parameters\n",
    "    threshold = 0.5  # Reduced threshold to ensure splitting\n",
    "    branching_factor = 3  \n",
    "    n_clusters = 2  \n",
    "\n",
    "    # Run BIRCH clustering\n",
    "    cf_tree, cluster_centroids = birch_clustering(data, threshold, branching_factor, n_clusters)\n",
    "\n",
    "    # Assign cluster to each data point\n",
    "    custom_labels = assign_clusters(data, cluster_centroids)\n",
    "\n",
    "    print(\"\\nCustom BIRCH Centroids for ai_global_index:\")\n",
    "    for i, centroid in enumerate(cluster_centroids):\n",
    "        print(f\"Cluster {i + 1}: {centroid}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom BIRCH Centroids for iris:\n",
      "Cluster 1: [-1.44593659  0.22214685 -1.3412724  -1.31297673]\n",
      "Cluster 2: [-0.90068117  1.03205722 -1.3412724  -1.31297673]\n",
      "Cluster 3: [-1.14301691 -0.1249576  -1.3412724  -1.31297673]\n",
      "\n",
      "Custom BIRCH Labels for iris: [1. 2. 0. 0. 1. 1. 0. 1. 2. 2. 1. 1. 2. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 1. 0. 0. 1. 1. 1. 2. 0. 1. 2. 0. 1. 1. 2. 0. 1. 1. 2. 1. 0.\n",
      " 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 1. 2. 1. 2. 2.\n",
      " 1. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 1. 2. 1.\n",
      " 1. 2. 2. 2. 1. 2.]\n",
      "Sklearn BIRCH Labels for iris: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 1 1 1 1 1 1 1 1 0 0 0 2 0 2 0 2 0 2 2 0 2 0 2 0 2 2 2 2 0 0 0 0\n",
      " 0 0 0 0 0 2 2 2 2 0 2 0 0 2 2 2 2 0 2 2 2 2 2 0 2 2 0 0 0 0 0 0 2 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "Adjusted Rand Index (ARI) for iris: 0.15834940845733184\n",
      "Custom BIRCH Centroids for ai_global_index:\n",
      "Cluster 1: [1.23656435 0.53537389 0.87554697 0.98274824 0.54928373 1.28819188\n",
      " 0.77055486 1.1096049 ]\n",
      "Cluster 2: [ 5.51273819  1.52171976 -0.11923719  4.8277688   4.42192742  0.74978683\n",
      "  6.74244485  5.07197419]\n",
      "Cluster 3: [-0.01941884  1.81991734  1.24225864  3.17316234  3.38205808  1.42106502\n",
      "  2.71974583  2.6001597 ]\n",
      "\n",
      "Custom BIRCH Labels for ai_global_index: [1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Sklearn BIRCH Labels for ai_global_index: [1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 2\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Adjusted Rand Index (ARI) for ai_global_index: 0.08932361449844689\n",
      "Custom BIRCH Centroids for global_earthquake:\n",
      "Cluster 1: [ 1.12567175  1.1465824   0.8384723  -0.17717986  0.68025673 -0.3928371\n",
      " -0.4152274   1.44378146 -0.59984796  0.00599163 -0.33747418  0.26175483\n",
      " -0.20523796 -0.32349139  0.2102272   2.04698145 -0.01088082  0.91936181]\n",
      "Cluster 2: [-0.43363518  1.04895758  0.73064771 -0.253103   -0.42111131 -0.3928371\n",
      "  2.40831892 -0.66064576  1.40094572 -0.59701003 -1.00314844 -1.40552401\n",
      " -1.0154126   1.12637012 -1.55194312 -1.30152262  1.04885764 -1.23638312]\n",
      "Cluster 3: [ 0.52056757  1.1007047   0.84471798  0.55388544 -0.42111131 -0.3928371\n",
      "  2.40831892  0.76723701  1.93154294 -0.495378   -0.33747418  2.37964956\n",
      "  0.82501498  0.05525287 -1.21663222 -0.78636815  0.70352862 -1.23638312]\n",
      "\n",
      "Custom BIRCH Labels for global_earthquake: [0. 0. 2. 1. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 2. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "Sklearn BIRCH Labels for global_earthquake: [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 2 0 1 1 1 1 1 1 1]\n",
      "Adjusted Rand Index (ARI) for global_earthquake: 0.1264062947613682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1419: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Extract numerical features\n",
    "    X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "    # Normalize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Run the custom BIRCH implementation\n",
    "    threshold = 0.01  # Adjust as needed\n",
    "    branching_factor = 10  # Adjust as needed\n",
    "    n_clusters = 3  # Adjust as needed\n",
    "\n",
    "    cf_tree, custom_centroids = birch_clustering(X, threshold, branching_factor, n_clusters)\n",
    "    custom_labels = np.zeros(len(X))  # Placeholder for custom labels\n",
    "    # Assign labels based on closest centroid (if needed)\n",
    "    for i, point in enumerate(X):\n",
    "        distances = [euclidean(point, centroid) for centroid in custom_centroids]\n",
    "        custom_labels[i] = np.argmin(distances)\n",
    "\n",
    "    print(f\"Custom BIRCH Centroids for {name}:\")\n",
    "    for i, centroid in enumerate(custom_centroids):\n",
    "        print(f\"Cluster {i + 1}: {centroid}\")\n",
    "    print(f\"\\nCustom BIRCH Labels for {name}: {custom_labels}\")\n",
    "    #print(custom_labels)\n",
    "\n",
    "    # Run the sklearn BIRCH implementation\n",
    "    sklearn_birch = SklearnBIRCH(threshold=threshold, branching_factor=branching_factor, n_clusters=n_clusters)\n",
    "    sklearn_labels = sklearn_birch.fit_predict(X)\n",
    "\n",
    "    print(f\"Sklearn BIRCH Labels for {name}: {sklearn_labels}\")\n",
    "\n",
    "    # Compare the results using Adjusted Rand Index (ARI)\n",
    "    ari_score = adjusted_rand_score(custom_labels, sklearn_labels)\n",
    "    results[name] = ari_score\n",
    "    print(f\"Adjusted Rand Index (ARI) for {name}: {ari_score}\")\n",
    "\n",
    "# Store results\n",
    "results = pd.Series(results)\n",
    "results.to_csv(\"./../results/birch_comparison.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
