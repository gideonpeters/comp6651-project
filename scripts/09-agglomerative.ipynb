{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchy Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering as SklearnAgglomerative\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": pd.read_csv(iris_dataset_path),\n",
    "    \"ai_global_index\": pd.read_csv(ai_global_index_path),\n",
    "    \"global_earthquake\": pd.read_csv(global_earthquake_data_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(iris_dataset_path)\n",
    "ai_global_index_df = pd.read_csv(ai_global_index_path)\n",
    "global_earthquake_data_df = pd.read_csv(global_earthquake_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if the paths exist\n",
    "print(os.path.exists(iris_dataset_path))      # Should be True\n",
    "print(os.path.exists(ai_global_index_path))  # Should be True\n",
    "print(os.path.exists(global_earthquake_data_path))  # Should be True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH Implementation (Based on our Algorithm - see report/Part-1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self, points):\n",
    "        self.points = points\n",
    "\n",
    "    def merge(self, other_cluster):\n",
    "        return Cluster(self.points + other_cluster.points)\n",
    "\n",
    "    def centroid(self):\n",
    "        return np.mean(self.points, axis=0)\n",
    "\n",
    "\n",
    "def compute_distance(cluster1, cluster2, linkage='single'):\n",
    "    if linkage == 'single':\n",
    "        return min(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'complete':\n",
    "        return max(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'average':\n",
    "        distances = [euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points]\n",
    "        return np.mean(distances)\n",
    "\n",
    "\n",
    "def hac_custom(data, k, linkage='single'):\n",
    "    clusters = [Cluster([point]) for point in data]\n",
    "\n",
    "    while len(clusters) > k:\n",
    "        min_distance = float('inf')\n",
    "        to_merge = (None, None)\n",
    "\n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(i + 1, len(clusters)):\n",
    "                distance = compute_distance(clusters[i], clusters[j], linkage)\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    to_merge = (i, j)\n",
    "\n",
    "        cluster1, cluster2 = to_merge\n",
    "        new_cluster = clusters[cluster1].merge(clusters[cluster2])\n",
    "        clusters = [c for idx, c in enumerate(clusters) if idx not in (cluster1, cluster2)]\n",
    "        clusters.append(new_cluster)\n",
    "\n",
    "    labels = np.zeros(len(data), dtype=int)\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point in cluster.points:\n",
    "            point_index = np.where((data == point).all(axis=1))[0][0]\n",
    "            labels[point_index] = cluster_idx\n",
    "\n",
    "    return clusters, labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    data = np.array([\n",
    "        [1, 2], [1, 4], [1, 0],\n",
    "        [10, 2], [10, 4], [10, 0]\n",
    "    ])\n",
    "\n",
    "    # Number of clusters\n",
    "    k = 2\n",
    "\n",
    "    # Run custom HAC\n",
    "    clusters, labels = hac_custom(data, k, linkage='average')\n",
    "\n",
    "    print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris dataset shape: (150, 5)\n",
      "Custom HAC Labels for iris: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2\n",
      " 0 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1]\n",
      "Sklearn HAC Labels for iris: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1\n",
      " 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1]\n",
      "Adjusted Rand Index (ARI) for iris: 0.963629008041949\n",
      "ai_global_index dataset shape: (62, 13)\n",
      "Custom HAC Labels for ai_global_index: [0 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "Sklearn HAC Labels for ai_global_index: [1 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Adjusted Rand Index (ARI) for ai_global_index: 1.0\n",
      "global_earthquake dataset shape: (1137, 43)\n",
      "Custom HAC Labels for global_earthquake: [2 2 2 2 2 2 2 2 2 2 2 0 0 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0\n",
      " 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 1 0 2 2 2 2 2 2 2]\n",
      "Sklearn HAC Labels for global_earthquake: [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "Adjusted Rand Index (ARI) for global_earthquake: 0.05761468710517783\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name} dataset shape: {df.shape}\")\n",
    "    # Drop missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Extract numerical features\n",
    "    X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "    # Normalize the data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # Number of clusters\n",
    "    k = 3  # Adjust as needed\n",
    "\n",
    "    # Run the custom HAC implementation\n",
    "    try:\n",
    "        custom_output = hac_custom(X, k, linkage='average')\n",
    "        if isinstance(custom_output, tuple):\n",
    "            custom_labels = custom_output[1]  # Extract the cluster labels\n",
    "        else:\n",
    "            custom_labels = custom_output\n",
    "\n",
    "        print(f\"Custom HAC Labels for {name}: {custom_labels}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom HAC for {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Run the sklearn HAC implementation\n",
    "    try:\n",
    "        sklearn_hac = SklearnAgglomerative(n_clusters=k, linkage='average')\n",
    "        sklearn_labels = sklearn_hac.fit_predict(X)\n",
    "        print(f\"Sklearn HAC Labels for {name}: {sklearn_labels}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in sklearn HAC for {name}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Compare the results using Adjusted Rand Index (ARI)\n",
    "    if custom_labels is not None and sklearn_labels is not None and len(custom_labels) == len(sklearn_labels):\n",
    "        ari_score = adjusted_rand_score(custom_labels, sklearn_labels)\n",
    "        results[name] = ari_score\n",
    "        print(f\"Adjusted Rand Index (ARI) for {name}: {ari_score}\")\n",
    "    else:\n",
    "        print(f\"Invalid clustering for {name}.\")\n",
    "\n",
    "# Store results\n",
    "results = pd.Series(results)\n",
    "results.to_csv(\"./../results/hac_comparison.csv\", header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
