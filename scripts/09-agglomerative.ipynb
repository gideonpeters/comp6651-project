{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Hierarchy Algorithm Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import heapq\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering as SklearnAgglomerative\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
    "    adjusted_rand_score, homogeneity_score, completeness_score, v_measure_score\n",
    ")\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./../datasets\"\n",
    "\n",
    "iris_dataset_path = dataset_path + \"/iris.csv\"                                         \n",
    "ai_global_index_path = dataset_path + \"/AI_index_db.csv\"\n",
    "global_earthquake_data_path = dataset_path + \"/earthquakes.csv\"\n",
    "\n",
    "datasets = {\n",
    "    \"iris\": pd.read_csv(iris_dataset_path),\n",
    "    \"ai_global_index\": pd.read_csv(ai_global_index_path),\n",
    "    \"global_earthquake\": pd.read_csv(global_earthquake_data_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv(iris_dataset_path)\n",
    "ai_global_index_df = pd.read_csv(ai_global_index_path)\n",
    "global_earthquake_data_df = pd.read_csv(global_earthquake_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIRCH Implementation (Based on our Algorithm - see report/Part-1.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [0 0 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "class Cluster:\n",
    "    def __init__(self, points, id):\n",
    "        self.points = np.array(points)  # Ensure points are stored as a numpy array\n",
    "        self.id = id  # Unique identifier for the cluster\n",
    "\n",
    "    def merge(self, other_cluster):\n",
    "        return Cluster(np.vstack([self.points, other_cluster.points]), self.id)\n",
    "\n",
    "    def centroid(self):\n",
    "        return np.mean(self.points, axis=0)\n",
    "\n",
    "\n",
    "def compute_distance(cluster1, cluster2, linkage='single'):\n",
    "    if linkage == 'single':\n",
    "        return min(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'complete':\n",
    "        return max(euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points)\n",
    "    elif linkage == 'average':\n",
    "        distances = [euclidean(p1, p2) for p1 in cluster1.points for p2 in cluster2.points]\n",
    "        return np.mean(distances)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid linkage method: {linkage}\")\n",
    "\n",
    "\n",
    "def hac_custom(data, k, linkage='single'):\n",
    "    n = len(data)\n",
    "    clusters = [Cluster([point], i) for i, point in enumerate(data)]\n",
    "    cluster_ids = [cluster.id for cluster in clusters]\n",
    "\n",
    "    # Initialize distance matrix\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist_matrix[i, j] = compute_distance(clusters[i], clusters[j], linkage)\n",
    "            dist_matrix[j, i] = dist_matrix[i, j]  # Symmetric matrix\n",
    "\n",
    "    # Use a priority queue (min-heap) to efficiently find the closest clusters\n",
    "    heap = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            heapq.heappush(heap, (dist_matrix[i, j], i, j))\n",
    "\n",
    "    # Track merge history for dendrogram (optional)\n",
    "    merge_history = []\n",
    "\n",
    "    while len(clusters) > k:\n",
    "        # Find the closest clusters\n",
    "        while True:\n",
    "            min_distance, i, j = heapq.heappop(heap)\n",
    "            if i in cluster_ids and j in cluster_ids:\n",
    "                break\n",
    "\n",
    "        # Merge the closest clusters\n",
    "        cluster1 = clusters[cluster_ids.index(i)]\n",
    "        cluster2 = clusters[cluster_ids.index(j)]\n",
    "        new_cluster = cluster1.merge(cluster2)\n",
    "        new_cluster_id = new_cluster.id\n",
    "\n",
    "        # Update clusters and cluster_ids\n",
    "        clusters = [c for c in clusters if c.id not in (i, j)]\n",
    "        clusters.append(new_cluster)\n",
    "        cluster_ids = [c.id for c in clusters]\n",
    "\n",
    "        # Update distance matrix for the new cluster\n",
    "        new_distances = []\n",
    "        for idx, cluster in enumerate(clusters[:-1]):\n",
    "            new_distance = compute_distance(cluster, new_cluster, linkage)\n",
    "            new_distances.append(new_distance)\n",
    "            heapq.heappush(heap, (new_distance, cluster.id, new_cluster_id))\n",
    "\n",
    "        # Record merge history (optional)\n",
    "        merge_history.append((i, j, min_distance))\n",
    "\n",
    "    # Assign labels based on cluster membership\n",
    "    labels = np.zeros(len(data), dtype=int)\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for point in cluster.points:\n",
    "            point_index = np.where((data == point).all(axis=1))[0]\n",
    "            if len(point_index) > 0:\n",
    "                labels[point_index[0]] = cluster_idx\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample dataset\n",
    "    data = np.array([\n",
    "        [1, 2], [1, 4], [1, 0],\n",
    "        [10, 2], [10, 4], [10, 0]\n",
    "    ])\n",
    "\n",
    "    # Number of clusters\n",
    "    k = 2\n",
    "\n",
    "    # Run custom HAC\n",
    "    labels = hac_custom(data, k, linkage='average')  # Only labels are returned\n",
    "\n",
    "    print(\"Cluster Labels:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: iris, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Testing n_clusters=2, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.554295982740022, 'Davies-Bouldin Index': 0.6208698585960091, 'Calinski-Harabasz Index': 215.7052309869769, 'Adjusted Rand Index (ARI)': 0.9463261709812779, 'Homogeneity': 0.8968976708307189, 'Completeness': 0.9106969052299235, 'V-Measure': 0.9037446161335015}\n",
      "Testing n_clusters=2, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.3725075642289735, 'Davies-Bouldin Index': 1.0854814725646726, 'Calinski-Harabasz Index': 101.17257274441651, 'Adjusted Rand Index (ARI)': 0.6162775009772758, 'Homogeneity': 0.5104148072213296, 'Completeness': 0.5112023543593543, 'V-Measure': 0.5108082772370551}\n",
      "Testing n_clusters=2, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.558164122034982, 'Davies-Bouldin Index': 0.6277342636151994, 'Calinski-Harabasz Index': 218.64763137110606, 'Adjusted Rand Index (ARI)': 0.9464323886639676, 'Homogeneity': 0.8900039242634512, 'Completeness': 0.8900039242634512, 'V-Measure': 0.8900039242634512}\n",
      "Testing n_clusters=3, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.4093816840090849, 'Davies-Bouldin Index': 1.294829095936943, 'Calinski-Harabasz Index': 122.1399281995958, 'Adjusted Rand Index (ARI)': 0.9643482223754433, 'Homogeneity': 0.9585682290998195, 'Completeness': 0.873329249581248, 'V-Measure': 0.91396563610634}\n",
      "Testing n_clusters=3, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.4007336333361126, 'Davies-Bouldin Index': 0.9201658532125047, 'Calinski-Harabasz Index': 169.56792022614573, 'Adjusted Rand Index (ARI)': 0.7501804859707241, 'Homogeneity': 0.7152825610089525, 'Completeness': 0.7132147438170686, 'V-Measure': 0.7142471557815767}\n",
      "Testing n_clusters=3, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.4697719747094586, 'Davies-Bouldin Index': 2.3524379371034088, 'Calinski-Harabasz Index': 109.16530394824953, 'Adjusted Rand Index (ARI)': 0.9379791694830855, 'Homogeneity': 0.8931820030677907, 'Completeness': 0.8378911504554581, 'V-Measure': 0.8646535758392264}\n",
      "Testing n_clusters=4, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.3339240812950297, 'Davies-Bouldin Index': 1.113207209065354, 'Calinski-Harabasz Index': 96.47344779672757, 'Adjusted Rand Index (ARI)': 0.9650833708877873, 'Homogeneity': 0.9634715623840627, 'Completeness': 0.8873522816196335, 'V-Measure': 0.9238466339484047}\n",
      "Testing n_clusters=4, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.3666434029991455, 'Davies-Bouldin Index': 2.388265184707012, 'Calinski-Harabasz Index': 108.66781051193479, 'Adjusted Rand Index (ARI)': 0.6314937763264293, 'Homogeneity': 0.6131329227581384, 'Completeness': 0.6752623460246255, 'V-Measure': 0.6426996215808128}\n",
      "Testing n_clusters=4, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.39551396871147537, 'Davies-Bouldin Index': 2.0446067934729903, 'Calinski-Harabasz Index': 91.57277240025945, 'Adjusted Rand Index (ARI)': 0.8976893370999552, 'Homogeneity': 0.9247238518593747, 'Completeness': 0.7750729737003462, 'V-Measure': 0.8433107474197978}\n",
      "\n",
      "Best Parameters for iris: {'n_clusters': 2, 'linkage': 'average'}\n",
      "Best Metrics for iris: {'Silhouette Score': 0.558164122034982, 'Davies-Bouldin Index': 0.6277342636151994, 'Calinski-Harabasz Index': 218.64763137110606, 'Adjusted Rand Index (ARI)': 0.9464323886639676, 'Homogeneity': 0.8900039242634512, 'Completeness': 0.8900039242634512, 'V-Measure': 0.8900039242634512}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def find_optimum_metrics(datasets):\n",
    "    results = {}\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"Dataset: {name}, Type: {type(df)}\")\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Extract numerical features\n",
    "        X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "        # Normalize the data\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Define parameter grid\n",
    "        param_grid = {\n",
    "            'n_clusters': [2, 3, 4],  # Adjust as needed\n",
    "            'linkage': ['single', 'complete', 'average'],  # Adjust as needed\n",
    "        }\n",
    "\n",
    "        # Initialize variables to store the best parameters and metrics\n",
    "        best_params = None\n",
    "        best_metrics = {\n",
    "            'Silhouette Score': -1,\n",
    "            'Davies-Bouldin Index': float('inf'),\n",
    "            'Calinski-Harabasz Index': -1,\n",
    "            'Adjusted Rand Index (ARI)': -1,\n",
    "            'Homogeneity': -1,\n",
    "            'Completeness': -1,\n",
    "            'V-Measure': -1,\n",
    "        }\n",
    "\n",
    "        # Iterate over all combinations of parameters\n",
    "        for n_clusters, linkage in product(param_grid['n_clusters'], param_grid['linkage']):\n",
    "            print(f\"Testing n_clusters={n_clusters}, linkage={linkage}\")\n",
    "\n",
    "            # Run custom Agglomerative Clustering\n",
    "            try:\n",
    "                custom_labels = hac_custom(X, n_clusters, linkage)  # Only labels are returned\n",
    "            except Exception as e:\n",
    "                print(f\"Error in custom Agglomerative Clustering for {name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Run Sklearn Agglomerative Clustering for comparison\n",
    "            try:\n",
    "                sklearn_agglo = SklearnAgglomerative(n_clusters=n_clusters, linkage=linkage)\n",
    "                sklearn_labels = sklearn_agglo.fit_predict(X)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in Sklearn Agglomerative Clustering for {name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Evaluate clustering using multiple metrics for CUSTOM Agglomerative Clustering\n",
    "            custom_metrics = {\n",
    "                'Silhouette Score': silhouette_score(X, custom_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(X, custom_labels),\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(X, custom_labels),\n",
    "                'Adjusted Rand Index (ARI)': adjusted_rand_score(custom_labels, sklearn_labels),\n",
    "                'Homogeneity': homogeneity_score(sklearn_labels, custom_labels),\n",
    "                'Completeness': completeness_score(sklearn_labels, custom_labels),\n",
    "                'V-Measure': v_measure_score(sklearn_labels, custom_labels),\n",
    "            }\n",
    "\n",
    "            # Evaluate clustering using multiple metrics for SKLEARN Agglomerative Clustering\n",
    "            sklearn_metrics = {\n",
    "                'Silhouette Score': silhouette_score(X, sklearn_labels),\n",
    "                'Davies-Bouldin Index': davies_bouldin_score(X, sklearn_labels),\n",
    "                'Calinski-Harabasz Index': calinski_harabasz_score(X, sklearn_labels),\n",
    "                'Adjusted Rand Index (ARI)': adjusted_rand_score(sklearn_labels, sklearn_labels),\n",
    "                'Homogeneity': homogeneity_score(sklearn_labels, sklearn_labels),\n",
    "                'Completeness': completeness_score(sklearn_labels, sklearn_labels),\n",
    "                'V-Measure': v_measure_score(sklearn_labels, sklearn_labels),\n",
    "            }\n",
    "\n",
    "            # Update the best parameters and metrics if the current metrics are better\n",
    "            if custom_metrics['Silhouette Score'] > best_metrics['Silhouette Score']:\n",
    "                best_params = {'n_clusters': n_clusters, 'linkage': linkage}\n",
    "                best_metrics = custom_metrics\n",
    "\n",
    "            print(f\"Metrics: {custom_metrics}\")\n",
    "\n",
    "        # Store the best parameters and metrics for this dataset\n",
    "        results[name] = {\n",
    "            'Best Parameters': best_params,\n",
    "            'Best Metrics': best_metrics,\n",
    "        }\n",
    "\n",
    "        # Print the best parameters and metrics for this dataset\n",
    "        print(f\"\\nBest Parameters for {name}: {best_params}\")\n",
    "        print(f\"Best Metrics for {name}: {best_metrics}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df = pd.DataFrame.from_dict({(i, j): results[i][j] \n",
    "                                        for i in results.keys() \n",
    "                                        for j in results[i].keys()}, \n",
    "                                       orient='index')\n",
    "    results_df.to_csv(\"./../results/optimum_metrics.csv\")\n",
    "\n",
    "    return results\n",
    "results = find_optimum_metrics({'iris': datasets.get('iris')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ai_global_index, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Testing n_clusters=2, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.7358407911622495, 'Davies-Bouldin Index': 0.1730858956119127, 'Calinski-Harabasz Index': 25.94792255616781, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=2, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.7358407911622495, 'Davies-Bouldin Index': 0.1730858956119127, 'Calinski-Harabasz Index': 25.94792255616781, 'Adjusted Rand Index (ARI)': 0.6479240364922734, 'Homogeneity': 0.4224762998723958, 'Completeness': 0.7291878731870094, 'V-Measure': 0.5349903240586071}\n",
      "Testing n_clusters=2, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.7358407911622495, 'Davies-Bouldin Index': 0.1730858956119127, 'Calinski-Harabasz Index': 25.94792255616781, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=3, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.5489778695449048, 'Davies-Bouldin Index': 0.2568680301556019, 'Calinski-Harabasz Index': 19.145660039267725, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=3, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.5489778695449048, 'Davies-Bouldin Index': 0.2568680301556019, 'Calinski-Harabasz Index': 19.145660039267725, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=3, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.5489778695449048, 'Davies-Bouldin Index': 0.2568680301556019, 'Calinski-Harabasz Index': 19.145660039267725, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=4, linkage=single\n",
      "Metrics: {'Silhouette Score': 0.15806272344988204, 'Davies-Bouldin Index': 0.4803954962974524, 'Calinski-Harabasz Index': 13.387747317142933, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "Testing n_clusters=4, linkage=complete\n",
      "Metrics: {'Silhouette Score': 0.31045745336819114, 'Davies-Bouldin Index': 0.3648007720579698, 'Calinski-Harabasz Index': 14.347002955481775, 'Adjusted Rand Index (ARI)': 0.23027451235794066, 'Homogeneity': 0.29186168833949283, 'Completeness': 0.6784636582919071, 'V-Measure': 0.4081467097061955}\n",
      "Testing n_clusters=4, linkage=average\n",
      "Metrics: {'Silhouette Score': 0.31045745336819114, 'Davies-Bouldin Index': 0.3648007720579698, 'Calinski-Harabasz Index': 14.347002955481775, 'Adjusted Rand Index (ARI)': 0.40128946838212015, 'Homogeneity': 0.413013737893319, 'Completeness': 0.6722959797563015, 'V-Measure': 0.5116833859575611}\n",
      "\n",
      "Best Parameters for ai_global_index: {'n_clusters': 2, 'linkage': 'single'}\n",
      "Best Metrics for ai_global_index: {'Silhouette Score': 0.7358407911622495, 'Davies-Bouldin Index': 0.1730858956119127, 'Calinski-Harabasz Index': 25.94792255616781, 'Adjusted Rand Index (ARI)': 1.0, 'Homogeneity': 1.0, 'Completeness': 1.0, 'V-Measure': 1.0}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = find_optimum_metrics({'ai_global_index': datasets.get('ai_global_index')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: global_earthquake, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Testing n_clusters=2, linkage=single\n",
      "Metrics: {'Silhouette Score': -0.016505019319068844, 'Davies-Bouldin Index': 8.044666961608051, 'Calinski-Harabasz Index': 0.8447911202843066, 'Adjusted Rand Index (ARI)': 0.005694936124365091, 'Homogeneity': 0.1524210492917446, 'Completeness': 0.016950164888053815, 'V-Measure': 0.030507686095497102}\n",
      "Testing n_clusters=2, linkage=complete\n",
      "Metrics: {'Silhouette Score': -0.01510202273855693, 'Davies-Bouldin Index': 6.780206918339515, 'Calinski-Harabasz Index': 1.1952233573772715, 'Adjusted Rand Index (ARI)': 0.005694936124365091, 'Homogeneity': 0.1524210492917446, 'Completeness': 0.016950164888053815, 'V-Measure': 0.030507686095497102}\n",
      "Testing n_clusters=2, linkage=average\n",
      "Metrics: {'Silhouette Score': -0.01510202273855693, 'Davies-Bouldin Index': 6.780206918339515, 'Calinski-Harabasz Index': 1.1952233573772715, 'Adjusted Rand Index (ARI)': -0.004771432969062644, 'Homogeneity': 0.11796306529376856, 'Completeness': 0.013118223609669978, 'V-Measure': 0.023610781998729072}\n",
      "Testing n_clusters=3, linkage=single\n",
      "Metrics: {'Silhouette Score': -0.014580287017155304, 'Davies-Bouldin Index': 5.079252524762651, 'Calinski-Harabasz Index': 4.582149569023972, 'Adjusted Rand Index (ARI)': 0.05761468710517783, 'Homogeneity': 0.5756028522528043, 'Completeness': 0.1164186314259387, 'V-Measure': 0.19366709815976388}\n",
      "Testing n_clusters=3, linkage=complete\n",
      "Metrics: {'Silhouette Score': -0.014580287017155304, 'Davies-Bouldin Index': 5.079252524762651, 'Calinski-Harabasz Index': 4.582149569023972, 'Adjusted Rand Index (ARI)': 0.05761468710517783, 'Homogeneity': 0.5756028522528043, 'Completeness': 0.1164186314259387, 'V-Measure': 0.19366709815976388}\n",
      "Testing n_clusters=3, linkage=average\n",
      "Metrics: {'Silhouette Score': -0.014580287017155304, 'Davies-Bouldin Index': 5.079252524762651, 'Calinski-Harabasz Index': 4.582149569023972, 'Adjusted Rand Index (ARI)': 0.05761468710517783, 'Homogeneity': 0.5756028522528043, 'Completeness': 0.1164186314259387, 'V-Measure': 0.19366709815976388}\n",
      "Testing n_clusters=4, linkage=single\n",
      "Metrics: {'Silhouette Score': -0.021434026355976295, 'Davies-Bouldin Index': 4.239450986181682, 'Calinski-Harabasz Index': 4.334520296671404, 'Adjusted Rand Index (ARI)': 0.10318883688377586, 'Homogeneity': 0.7166581796948578, 'Completeness': 0.19940255677007498, 'V-Measure': 0.311995630143052}\n",
      "Testing n_clusters=4, linkage=complete\n",
      "Metrics: {'Silhouette Score': -0.004678091189105306, 'Davies-Bouldin Index': 5.970860896474623, 'Calinski-Harabasz Index': 5.270948092458718, 'Adjusted Rand Index (ARI)': 0.1694795886790708, 'Homogeneity': 0.6510107290237204, 'Completeness': 0.2686438749507957, 'V-Measure': 0.38033854041211523}\n",
      "Testing n_clusters=4, linkage=average\n",
      "Metrics: {'Silhouette Score': -0.003395844377488791, 'Davies-Bouldin Index': 6.475043198667912, 'Calinski-Harabasz Index': 6.100581382495262, 'Adjusted Rand Index (ARI)': 0.21018955702785674, 'Homogeneity': 0.8267508783197015, 'Completeness': 0.3292713478548897, 'V-Measure': 0.47096910402037295}\n",
      "\n",
      "Best Parameters for global_earthquake: {'n_clusters': 4, 'linkage': 'average'}\n",
      "Best Metrics for global_earthquake: {'Silhouette Score': -0.003395844377488791, 'Davies-Bouldin Index': 6.475043198667912, 'Calinski-Harabasz Index': 6.100581382495262, 'Adjusted Rand Index (ARI)': 0.21018955702785674, 'Homogeneity': 0.8267508783197015, 'Completeness': 0.3292713478548897, 'V-Measure': 0.47096910402037295}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = find_optimum_metrics({'global_earthquake': datasets.get('global_earthquake')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: iris, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Custom Agglomerative Clustering Labels for iris: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1]\n",
      "Sklearn Agglomerative Clustering Labels for iris: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "\n",
      "Metrics for iris:\n",
      "Custom Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 0.9464323886639676\n",
      "Silhouette Score: 0.558164122034982\n",
      "Davies-Bouldin Index: 0.6277342636151994\n",
      "Calinski-Harabasz Index: 218.64763137110606\n",
      "Homogeneity: 0.8900039242634512\n",
      "Completeness: 0.8900039242634512\n",
      "V-Measure: 0.8900039242634512\n",
      "\n",
      "Sklearn Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 1.0\n",
      "Silhouette Score: 0.580184463257396\n",
      "Davies-Bouldin Index: 0.5975546650809878\n",
      "Calinski-Harabasz Index: 248.90342786485118\n",
      "Homogeneity: 1.0\n",
      "Completeness: 1.0\n",
      "V-Measure: 1.0\n",
      "Dataset: ai_global_index, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Custom Agglomerative Clustering Labels for ai_global_index: [0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Sklearn Agglomerative Clustering Labels for ai_global_index: [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "Metrics for ai_global_index:\n",
      "Custom Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 1.0\n",
      "Silhouette Score: 0.7358407911622495\n",
      "Davies-Bouldin Index: 0.1730858956119127\n",
      "Calinski-Harabasz Index: 25.94792255616781\n",
      "Homogeneity: 1.0\n",
      "Completeness: 1.0\n",
      "V-Measure: 1.0\n",
      "\n",
      "Sklearn Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 1.0\n",
      "Silhouette Score: 0.7358407911622495\n",
      "Davies-Bouldin Index: 0.1730858956119127\n",
      "Calinski-Harabasz Index: 25.94792255616781\n",
      "Homogeneity: 1.0\n",
      "Completeness: 1.0\n",
      "V-Measure: 1.0\n",
      "Dataset: global_earthquake, Type: <class 'pandas.core.frame.DataFrame'>\n",
      "Custom Agglomerative Clustering Labels for global_earthquake: [3 3 3 3 3 3 3 3 3 3 3 0 0 0 3 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0 0 0 0 0\n",
      " 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 1 0 3 3 3 3 2 2 2]\n",
      "Sklearn Agglomerative Clustering Labels for global_earthquake: [1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 3 1 1 1 1 1 0 0 0]\n",
      "\n",
      "Metrics for global_earthquake:\n",
      "Custom Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 0.21018955702785674\n",
      "Silhouette Score: -0.003395844377488791\n",
      "Davies-Bouldin Index: 6.475043198667912\n",
      "Calinski-Harabasz Index: 6.100581382495262\n",
      "Homogeneity: 0.8267508783197015\n",
      "Completeness: 0.3292713478548897\n",
      "V-Measure: 0.47096910402037295\n",
      "\n",
      "Sklearn Agglomerative Metrics:\n",
      "Adjusted Rand Index (ARI): 1.0\n",
      "Silhouette Score: 0.3438511072918944\n",
      "Davies-Bouldin Index: 0.7201294985212869\n",
      "Calinski-Harabasz Index: 10.090403651819594\n",
      "Homogeneity: 1.0\n",
      "Completeness: 1.0\n",
      "V-Measure: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define the comparison function\n",
    "def compare_agglo_implementations(datasets, best_parameters):\n",
    "    all_results = {}\n",
    "\n",
    "    for name, df in datasets.items():\n",
    "        print(f\"Dataset: {name}, Type: {type(df)}\")\n",
    "\n",
    "        df = df.dropna()\n",
    "\n",
    "        # Extract numerical features\n",
    "        X = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "        # Normalize the data\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "\n",
    "        # Get the best parameters for this dataset\n",
    "        n_clusters = best_parameters[name]['n_clusters']\n",
    "        linkage = best_parameters[name]['linkage']\n",
    "\n",
    "        # Run the custom Agglomerative Clustering implementation\n",
    "        try:\n",
    "            custom_labels = hac_custom(X, n_clusters, linkage)\n",
    "            print(f\"Custom Agglomerative Clustering Labels for {name}: {custom_labels}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in custom Agglomerative Clustering for {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Run the Sklearn Agglomerative Clustering implementation\n",
    "        try:\n",
    "            sklearn_agglo = SklearnAgglomerative(n_clusters=n_clusters, linkage=linkage)\n",
    "            sklearn_labels = sklearn_agglo.fit_predict(X)\n",
    "            print(f\"Sklearn Agglomerative Clustering Labels for {name}: {sklearn_labels}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Sklearn Agglomerative Clustering for {name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Evaluate clustering using multiple metrics for CUSTOM Agglomerative Clustering\n",
    "        custom_metrics = {\n",
    "            \"Adjusted Rand Index (ARI)\": adjusted_rand_score(custom_labels, sklearn_labels),\n",
    "            \"Silhouette Score\": silhouette_score(X, custom_labels),\n",
    "            \"Davies-Bouldin Index\": davies_bouldin_score(X, custom_labels),\n",
    "            \"Calinski-Harabasz Index\": calinski_harabasz_score(X, custom_labels),\n",
    "            \"Homogeneity\": homogeneity_score(sklearn_labels, custom_labels),\n",
    "            \"Completeness\": completeness_score(sklearn_labels, custom_labels),\n",
    "            \"V-Measure\": v_measure_score(sklearn_labels, custom_labels),\n",
    "        }\n",
    "\n",
    "        # Evaluate clustering using multiple metrics for SKLEARN Agglomerative Clustering\n",
    "        sklearn_metrics = {\n",
    "            \"Adjusted Rand Index (ARI)\": adjusted_rand_score(sklearn_labels, sklearn_labels),\n",
    "            \"Silhouette Score\": silhouette_score(X, sklearn_labels),\n",
    "            \"Davies-Bouldin Index\": davies_bouldin_score(X, sklearn_labels),\n",
    "            \"Calinski-Harabasz Index\": calinski_harabasz_score(X, sklearn_labels),\n",
    "            \"Homogeneity\": homogeneity_score(sklearn_labels, sklearn_labels),\n",
    "            \"Completeness\": completeness_score(sklearn_labels, sklearn_labels),\n",
    "            \"V-Measure\": v_measure_score(sklearn_labels, sklearn_labels),\n",
    "        }\n",
    "\n",
    "        # Store results for both implementations\n",
    "        all_results[name] = {\n",
    "            \"Custom Agglomerative\": custom_metrics,\n",
    "            \"Sklearn Agglomerative\": sklearn_metrics,\n",
    "        }\n",
    "\n",
    "        # Print metrics for both implementations\n",
    "        print(f\"\\nMetrics for {name}:\")\n",
    "        print(\"Custom Agglomerative Metrics:\")\n",
    "        for metric_name, value in custom_metrics.items():\n",
    "            print(f\"{metric_name}: {value}\")\n",
    "\n",
    "        print(\"\\nSklearn Agglomerative Metrics:\")\n",
    "        for metric_name, value in sklearn_metrics.items():\n",
    "            print(f\"{metric_name}: {value}\")\n",
    "\n",
    "    # Store all results in a structured format\n",
    "    results_df = pd.DataFrame.from_dict(\n",
    "        {(i, j): all_results[i][j] for i in all_results.keys() for j in all_results[i].keys()},\n",
    "        orient='index'\n",
    "    )\n",
    "    results_df.to_csv(\"./../results/agglo_comparison_metrics.csv\")\n",
    "\n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your datasets\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    iris = load_iris()\n",
    "    iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "\n",
    "    # Define the best parameters for each dataset\n",
    "    best_parameters = {\n",
    "        'iris': {'n_clusters': 2, 'linkage': 'average'},\n",
    "        'ai_global_index': {'n_clusters': 2, 'linkage': 'single'},\n",
    "        'global_earthquake': {'n_clusters': 4, 'linkage': 'average'},\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Compare custom Agglomerative and Sklearn Agglomerative for all datasets\n",
    "    results = compare_agglo_implementations(datasets, best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
